[platform]
r000u08l03=slurm
r000u07l02=slurm
r000u06l01=slurm

maxUserSessions=20
usetunnel=y
nodepostfix=-hfi

[vnc_menu]

fluxbox_turbovnc_vnc=fluxbox|use turbovnc with fluxbox
udocker_turbovnc_vnc=fluxbox|use turbovnc with udocker
singularity_turbovnc_vnc=fluxbox|use turbovnc with singularity

[vnc_authfile]
vnc= -rfbauth $RCM_JOBLOG.pwd

[vnc_foreground]
vnc= -fg

[vnc_geometry]
vnc= -geometry $RCM_GEOMETRY

[vnc_startfile]
default=
fluxbox= -xstartup ${RCM_HOME}/bin/config/xstartup.fluxbox
udocker= -xstartup ${RCM_HOME}/bin/config/xstartup.udocker
singularity= -xstartup ${RCM_HOME}/bin/config/xstartup.singularity

[vnc_command]
vnc=vncserver $vnc_foreground $vnc_geometry $vnc_authfile $vnc_startfile

[vnc_setup]
turbovnc=module load rcm


[jobscript]

4core_18_Gb_3h_slurm=#!/bin/bash
 #SBATCH --partition bdw_usr_dbg
 #SBATCH --qos=qos_rcm
 #SBATCH --time=$RCM_WALLTIME
 #SBATCH --job-name=$RCM_SESSIONID
 #SBATCH --output $RCM_JOBLOG
 #SBATCH -N 1 -n 6 --mem=18GB

 $RCM_MODULE_SETUP
 $RCM_CLEANPIDS
 $RCM_VNCSERVER > $RCM_JOBLOG.vnc 2>&1

12core_40_Gb_3h_slurm=#!/bin/bash
 #SBATCH --partition bdw_usr_dbg
 #SBATCH --qos=qos_rcm
 #SBATCH --time=$RCM_WALLTIME
 #SBATCH --job-name=$RCM_SESSIONID
 #SBATCH --output $RCM_JOBLOG
 #SBATCH -N 1 -n 12 --mem=40GB

 $RCM_MODULE_SETUP
 $RCM_CLEANPIDS
 $RCM_VNCSERVER > $RCM_JOBLOG.vnc 2>&1

4core_18_Gb_1h_slurm=#!/bin/bash
 #SBATCH --partition bdw_usr_dbg
 #SBATCH --qos=qos_rcm
 #SBATCH --time=$RCM_WALLTIME
 #SBATCH --job-name=$RCM_SESSIONID
 #SBATCH --output $RCM_JOBLOG
 #SBATCH -N 1 -n 6 --mem=18GB

 $RCM_MODULE_SETUP
 $RCM_CLEANPIDS
 $RCM_VNCSERVER > $RCM_JOBLOG.vnc 2>&1

4core_18_Gb_20min_skl=#!/bin/bash
 #SBATCH --partition skl_usr_dbg
 #SBATCH --qos qos_rcm
 ##SBATCH ${RCM_QSUBPAR_A}
 #SBATCH --time=$RCM_WALLTIME
 #SBATCH --job-name=$RCM_SESSIONID
 #SBATCH --output $RCM_JOBLOG
 #SBATCH -N 1 -n 6 --mem=18GB

 $RCM_MODULE_SETUP
 $RCM_CLEANPIDS
 $RCM_VNCSERVER > $RCM_JOBLOG.vnc 2>&1

ssh=#!/bin/bash
 $RCM_MODULE_SETUP
 $RCM_CLEANPIDS
 $RCM_VNCSERVER > $RCM_JOBLOG.vnc 2>&1
 cat `ls -tr ~/.vnc/*.pid | tail -1`


[testjobscript]
4core_18_Gb_3h_slurm=sbatch --partition bdw_usr_dbg  --time=00:00:01 --output /dev/null  ${RCM_QSUBPAR_A}  -N 1 -n 1   --wrap "/usr/bin/echo "
12core_40_Gb_3h_slurm=sbatch --partition bdw_usr_dbg  --time=00:00:01 --output /dev/null  ${RCM_QSUBPAR_A}  -N 1 -n 1   --wrap "/usr/bin/echo "
4core_18_Gb_1h_slurm=sbatch --partition bdw_usr_dbg  --time=00:00:01 --output /dev/null  ${RCM_QSUBPAR_A}  -N 1 -n 1   --wrap "/usr/bin/echo"
4core_18_Gb_20min_skl=sbatch --partition skl_usr_dbg  --time=00:00:01 --output /dev/null  -J test-slurm- --qos qos_rcm  -N 1 -n 1   --wrap "/usr/bin/echo"

[walltimelimit]
4core_18_Gb_3h_slurm=2:58:00
12core_40_Gb_3h_slurm=2:58:00
4core_18_Gb_1h_slurm=1:00:00
4core_18_Gb_20min_skl=3:00:00

[130.186.17]
#login02.galileo.cineca.it=login.galileo.cineca.it
r000u06l01-fe.marconi.cineca.it=login.marconi.cineca.it
r000u07l02-fe.marconi.cineca.it=login.marconi.cineca.it
r000u08l03-fe.marconi.cineca.it=login.marconi.cineca.it

