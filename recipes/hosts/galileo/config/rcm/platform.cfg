[platform]
node167=slurm
node165=slurm
node166=slurm
nodepostfix=-ib1
maxUserSessions=10
usetunnel=y

[vnc_menu]
#Here is the list of displayed alternatives, underscore separate components
#that maps to following definitions, in a hierichical mapping
kde_turbovnc_vnc=kde|use turbovnc with kde
fluxbox_turbovnc_vnc=fluxbox|use turbovnc with fluxbox
#fluxbox_TigerVNC_vnc=tigervnc|use tigervnc server for compatibility
#gnomegl_turbovnc_vnc=gnomegl|use turbovnc with gnome shell (accelerated)

#fluxbox_TigerVNC_vnc=tigervnc|use tigervnc server for compatibility


[vnc_authfile]
#this means that all menu entries (ending vnc) expand this authfile definition
vnc= -rfbauth $RCM_JOBLOG.pwd

[vnc_foreground]
#this means that all menu entries (ending vnc) expand this to define foreground
#run mode, so quitting window manager should quit the session job 
vnc= -fg

[vnc_geometry]
#this means that all menu entries (ending vnc) expand this to define 
#window geometry at startup
vnc= -geometry $RCM_GEOMETRY

[vnc_startfile]
#this is the startup file definiton, different from fluxbox and gnomegl entries
fluxbox= ${RCM_HOME}/bin/config/xstartup.fluxbox

#startfiles can be customized depending on node
fluxbox@node170= ${RCM_HOME}/bin/config/xstartup_eni.fluxbox
fluxbox@node171= ${RCM_HOME}/bin/config/xstartup_eni.fluxbox
fluxbox@node172= ${RCM_HOME}/bin/config/xstartup_eni.fluxbox

kde=          ${RCM_HOME}/bin/config/xstartup.kde
kde@node170=  ${RCM_HOME}/bin/config/xstartup_eni.kde
kde@node171=  ${RCM_HOME}/bin/config/xstartup_eni.kde
kde@node172=  ${RCM_HOME}/bin/config/xstartup_eni.kde

mwm=           ${RCM_HOME}/bin/config/xstartup.mwm
metacity=      ${RCM_HOME}/bin/config/xstartup.metacity

gnomegl= ${RCM_HOME}/bin/config/xstartup.gnomegl
#eni#gnomegl= ${RCM_HOME_BAK}/bin/config/xstartup.gnomegl
#gnomegl= -xstartup /cineca/prod/tools/rcm/1.2/none/bin/config/xstartup.gnomegl

[vnc_option3d]
vnc=
gnomegl= -3dwm


[vnc_optionxstart]
turbovnc= -xstartup
TigerVNC= ${RCM_XSTARTUP}


[vnc_command]
#this is the vncserver sart command line, where previously defined entries 
#prefixed with $ are expanded from above definitions
#vnc is used for all menu entries except for gnomegl that override it 
#as gnomegl comes first in the menu entry label separated by _

#old#vnc=vncserver $vnc_foreground $vnc_geometry $vnc_authfile $vnc_startfile
vnc=vncserver $vnc_option3d $vnc_foreground $vnc_geometry $vnc_authfile
turbovnc=vncserver $vnc_option3d $vnc_foreground $vnc_geometry $vnc_authfile $vnc_optionxstart $vnc_startfile
TigerVNC=if [ "x" == "x$(vncserver -h 2>&1 | grep '\-xstartup')" ]; then cp $vnc_startfile $HOME/.vnc/xstartup;  export RCM_XSTARTUP=''; else export RCM_XSTARTUP='-xstartup $vnc_startfile';  fi; vncserver $vnc_option3d $vnc_foreground $vnc_geometry $vnc_authfile $vnc_optionxstart

[vnc_setup]
#This define the different setup needed to make the $vnc_command work
#for gnomegl the virtualgl module is needed
#for TigerVNC the turbovnc module is unloaded to let system installed vncserver
#be runned 

#old#turbovnc=source /cineca/prod/environment/module/3.1.6/none/init/bash; module purge; module load profile/advanced  turbovnc/svn rcm ; if xdpyinfo -d :0 > /dev/null ; then   module load virtualgl; fi
turbovnc=module load rcm
#eni#gnomegl=export RCM_HOME_BAK=${RCM_HOME}; module purge;  module load turbovnc virtualgl
TigerVNC=module load rcm; module unload turbovnc


#also vnc setup can be customized depending on nodes
turbovnc@node170=source /developers/devenv/prod/opt/environment/module/3.1.6/none/init/bash; module purge; module load autoload turbovnc/svn ; if xdpyinfo -d :0 > /dev/null ; then   module load virtualgl; fi
turbovnc@node171=source /developers/devenv/prod/opt/environment/module/3.1.6/none/init/bash; module purge; module load autoload turbovnc/svn ; if xdpyinfo -d :0 > /dev/null ; then   module load virtualgl; fi
turbovnc@node172=source /developers/devenv/prod/opt/environment/module/3.1.6/none/init/bash; module purge; module load autoload turbovnc/svn ; if xdpyinfo -d :0 > /dev/null ; then   module load virtualgl; fi

[jobscript]
#These are possible sessions to be presented as alternatives, $variavles are 
#expanded into the job that is submitted
# $RCM_MODULE_SETUP <-- $vnc_setup
# $RCM_VNCSERVER <-- $vnc_command

#last entry is for the dummy ssh scheduler

#PBS sessios
light_2Gb_1cor=#!/bin/bash 
 #PBS -l walltime=$RCM_WALLTIME 
 #PBS -N $RCM_SESSIONID 
 #PBS -o $RCM_JOBLOG   
 #PBS -l select=1:ncpus=1:mem=2Gb
 #PBS -j oe 
 #PBS -q visualrcm 
 $RCM_MODULE_SETUP
 $RCM_CLEANPIDS 
 $RCM_VNCSERVER > $RCM_JOBLOG.vnc 2>&1

medium_8Gb_1core=#!/bin/bash 
 #PBS -l walltime=$RCM_WALLTIME 
 #PBS -N $RCM_SESSIONID 
 #PBS -o $RCM_JOBLOG   
 #PBS -l select=1:ncpus=1:mem=8Gb
 #PBS -j oe 
 #PBS -q visualrcm 
 $RCM_MODULE_SETUP
 $RCM_CLEANPIDS 
 $RCM_VNCSERVER > $RCM_JOBLOG.vnc 2>&1

med_16Gb_2core=#!/bin/bash 
 #PBS -l walltime=$RCM_WALLTIME 
 #PBS -N $RCM_SESSIONID 
 #PBS -o $RCM_JOBLOG   
 #PBS -l select=1:ncpus=2:mem=16Gb
 #PBS -j oe 
 #PBS -q visualrcm 
 $RCM_MODULE_SETUP
 $RCM_CLEANPIDS
 $RCM_VNCSERVER > $RCM_JOBLOG.vnc 2>&1

 
cpu_8Gb_8core=#!/bin/bash 
 #PBS -l walltime=$RCM_WALLTIME 
 #PBS -N $RCM_SESSIONID 
 #PBS -o $RCM_JOBLOG   
 #PBS -l select=1:ncpus=8:mem=8Gb
 #PBS -j oe 
 #PBS -q visualrcm 
 $RCM_MODULE_SETUP
 $RCM_CLEANPIDS
 $RCM_VNCSERVER > $RCM_JOBLOG.vnc 2>&1

alarge_32Gb_4core=#!/bin/bash 
 #PBS -l walltime=$RCM_WALLTIME 
 #PBS -N $RCM_SESSIONID 
 #PBS -o $RCM_JOBLOG   
 #PBS -l select=1:ncpus=4:mem=32Gb
 #PBS -j oe 
 #PBS -q visualrcm 
 $RCM_MODULE_SETUP
 $RCM_CLEANPIDS 
 $RCM_VNCSERVER > $RCM_JOBLOG.vnc 2>&1

xtralarge_64Gb_8c=#!/bin/bash 
 #PBS -l walltime=$RCM_WALLTIME 
 #PBS -N $RCM_SESSIONID 
 #PBS -o $RCM_JOBLOG   
 #PBS -l select=1:ncpus=8:mem=60Gb
 #PBS -j oe 
 #PBS -q visualrcm 
 $RCM_MODULE_SETUP
 $RCM_CLEANPIDS 
 $RCM_VNCSERVER > $RCM_JOBLOG.vnc 2>&1
 
zfull_120Gb_16c=#!/bin/bash 
 #PBS -l walltime=$RCM_WALLTIME 
 #PBS -N $RCM_SESSIONID 
 #PBS -o $RCM_JOBLOG   
 #PBS -l select=1:ncpus=16:mem=120Gb
 #PBS -j oe 
 #PBS -q visualrcm 
 $RCM_MODULE_SETUP
 $RCM_CLEANPIDS 
 $RCM_VNCSERVER > $RCM_JOBLOG.vnc 2>&1
 
 
dcv_visual=#!/bin/bash 
 #PBS -l walltime=$RCM_WALLTIME
 ##PBS -l "$RCM_QUEUEPARAMETER"
 #PBS -N $RCM_SESSIONID 
 #PBS -o $RCM_JOBLOG   
 #PBS -l select=1:ncpus=1
 ##$RCM_DIRECTIVE_W
 ##PBS -A cin_visual
 #PBS -j oe 
 #PBS -q dcv_visual 
 $RCM_MODULE_SETUP
 $RCM_CLEANPIDS 
 $RCM_VNCSERVER  > $RCM_JOBLOG.vnc 2>&1

#slurm sessions
1core_2_Gb_3h=#!/bin/bash
 #SBATCH --time=$RCM_WALLTIME
 #SBATCH --job-name=$RCM_SESSIONID
 #SBATCH --output $RCM_JOBLOG
 #SBATCH -N 1 -n 1 --mem=2GB gres=gpu:kepler:1
 #SBATCH ${RCM_QSUBPAR_A}
 #SBATCH --partition=gll_all_serial
 $RCM_MODULE_SETUP
 $RCM_CLEANPIDS
 $RCM_VNCSERVER > $RCM_JOBLOG.vnc 2>&1


4core_24_Gb_12h=#!/bin/bash
 #SBATCH --time=$RCM_WALLTIME
 #SBATCH --job-name=$RCM_SESSIONID
 #SBATCH --output $RCM_JOBLOG
 #SBATCH -N 1 -n 4 --mem=24GB
 #SBATCH ${RCM_QSUBPAR_A}
 #SBATCH --partition=gll_all_serial
 $RCM_MODULE_SETUP
 $RCM_CLEANPIDS
 $RCM_VNCSERVER > $RCM_JOBLOG.vnc 2>&1



 
4core_24_Gb_12h_gpuprod=#!/bin/bash
 #SBATCH --time=$RCM_WALLTIME
 #SBATCH --job-name=$RCM_SESSIONID
 #SBATCH --output $RCM_JOBLOG
 #SBATCH -N 1 -n 4 --mem=24GB --ntasks-per-node=4  --partition=gll_usr_gpudev --gres=gpu:kepler:1
 #SBATCH ${RCM_QSUBPAR_A}
 #SBATCH --partition=gll_usr_gpuprod
 $RCM_MODULE_SETUP
 $RCM_CLEANPIDS
 $RCM_VNCSERVER > $RCM_JOBLOG.vnc 2>&1



fullnode_3h_gpuprod=#!/bin/bash
 #SBATCH --time=$RCM_WALLTIME
 #SBATCH --job-name=$RCM_SESSIONID
 #SBATCH --output $RCM_JOBLOG
 #SBATCH -N 1 --ntasks-per-node=16  --partition=gll_usr_gpuprod  --gres=gpu:kepler:4
 #SBATCH ${RCM_QSUBPAR_A}
 #SBATCH --partition=gll_usr_gpuprod
 $RCM_MODULE_SETUP
 $RCM_CLEANPIDS
 $RCM_VNCSERVER > $RCM_JOBLOG.vnc 2>&1



fullnode_3h_gpudev=#!/bin/bash
 #SBATCH --time=$RCM_WALLTIME
 #SBATCH --job-name=$RCM_SESSIONID
 #SBATCH --output $RCM_JOBLOG
 #SBATCH -N 1 --ntasks-per-node=16  --partition=gll_usr_gpudev --qos=gll_qos_gpudev  --gres=gpu:kepler:4
 #SBATCH ${RCM_QSUBPAR_A}
 #SBATCH --partition=gll_usr_gpuprod
 $RCM_MODULE_SETUP
 $RCM_CLEANPIDS
 $RCM_VNCSERVER > $RCM_JOBLOG.vnc 2>&1



fullnode_3h_prod=#!/bin/bash
 #SBATCH --time=$RCM_WALLTIME
 #SBATCH --job-name=$RCM_SESSIONID
 #SBATCH --output $RCM_JOBLOG
 #SBATCH -N 1 -n 36
 #SBATCH ${RCM_QSUBPAR_A}
 #SBATCH --partition=gll_usr_prod
 $RCM_MODULE_SETUP
 $RCM_CLEANPIDS
 $RCM_VNCSERVER > $RCM_JOBLOG.vnc 2>&1



course=#!/bin/bash
 #SBATCH --time=$RCM_WALLTIME
 #SBATCH --job-name=$RCM_SESSIONID
 #SBATCH --output $RCM_JOBLOG
 #SBATCH -N 1 -n 1 --mem=2GB
 #SBATCH -A train_cgpB2018
 #SBATCH --partition=gll_usr_gpuprod
 #SBATCH --reservation=s_tra_cgpB2018
 $RCM_MODULE_SETUP
 $RCM_CLEANPIDS
 $RCM_VNCSERVER > $RCM_JOBLOG.vnc 2>&1




course_1gpu=#!/bin/bash
 #SBATCH --time=$RCM_WALLTIME
 #SBATCH --job-name=$RCM_SESSIONID
 #SBATCH --output $RCM_JOBLOG
 #SBATCH -N 1 -n 1 --mem=2GB
 #SBATCH -A train_cgpB2018
 #SBATCH --partition=gll_usr_gpuprod
 #SBATCH --reservation=s_tra_cgpB2018
 #SBATCH --ntasks-per-node=1  --gres=gpu:kepler:1
 $RCM_MODULE_SETUP
 $RCM_CLEANPIDS
 $RCM_VNCSERVER > $RCM_JOBLOG.vnc 2>&1



course_2gpu=#!/bin/bash
 #SBATCH --time=$RCM_WALLTIME
 #SBATCH --job-name=$RCM_SESSIONID
 #SBATCH --output $RCM_JOBLOG
 #SBATCH -N 1 -n 2 --mem=8GB
 #SBATCH -A train_cgpB2018
 #SBATCH --partition=gll_usr_gpuprod
 #SBATCH --reservation=s_tra_cgpB2018
 #SBATCH --ntasks-per-node=2  --gres=gpu:kepler:2
 $RCM_MODULE_SETUP
 $RCM_CLEANPIDS
 $RCM_VNCSERVER > $RCM_JOBLOG.vnc 2>&1



course_4gpu=#!/bin/bash
 #SBATCH --time=$RCM_WALLTIME
 #SBATCH --job-name=$RCM_SESSIONID
 #SBATCH --output $RCM_JOBLOG
 #SBATCH -N 1 -n 4 --mem=32GB
 #SBATCH -A train_cgpB2018
 #SBATCH --partition=gll_usr_gpuprod
 #SBATCH --reservation=s_tra_cgpB2018
 #SBATCH --ntasks-per-node=4  --gres=gpu:kepler:4
 $RCM_MODULE_SETUP
 $RCM_CLEANPIDS
 $RCM_VNCSERVER > $RCM_JOBLOG.vnc 2>&1



#this is the session definition for ssh start jobs 
ssh=#!/bin/bash
 $RCM_MODULE_SETUP
 $RCM_CLEANPIDS
 $RCM_VNCSERVER > $RCM_JOBLOG.vnc 2>&1
 cat `ls -tr ~/.vnc/*.pid | tail -1`

 
[testjobscript]
#visual=qsub -l walltime=00:00:01 -l select=1 -o /dev/null -e /dev/null -q visual -- echo
light_2Gb_1cor=qsub -l walltime=00:00:01 -o /dev/null -e /dev/null -W sandbox=PRIVATE -k oe -l select=1:mem=100Mb -q visualrcm  -- /usr/bin/echo
medium_8Gb_1core=qsub -l walltime=00:00:01 -o /dev/null -e /dev/null -W sandbox=PRIVATE -k oe -l select=1:mem=100Mb -q visualrcm -- /usr/bin/echo
med_16Gb_2core=qsub -l walltime=00:00:01 -o /dev/null -e /dev/null -W sandbox=PRIVATE -k oe -l select=1:mem=100Mb -q visualrcm -- /usr/bin/echo
#cpu_8Gb_8core=qsub -l walltime=00:00:01 -o /dev/null -e /dev/null -l select=1 -q visualrcm -- /usr/bin/echo
xtralarge_64Gb_8c=qsub -l walltime=00:00:01 -o /dev/null -e /dev/null -W sandbox=PRIVATE -k oe -l select=1:mem=100Mb -q visualrcm -- echo
alarge_32Gb_4core=qsub -l walltime=00:00:01 -o /dev/null -e /dev/null -W sandbox=PRIVATE -k oe -l select=1:mem=100Mb -q visualrcm -- /usr/bin/echo
zfull_120Gb_16c=qsub -l walltime=00:00:01 -o /dev/null -e /dev/null -W sandbox=PRIVATE -k oe -l select=1:mem=100Mb -q visualrcm -- echo
#visualrcm16=qsub -l walltime=00:00:01 -o /dev/null -e /dev/null -l select=1 -q visualrcm -- echo
#dcv_visual=qsub -l walltime=00:00:01 -o /dev/null -e /dev/null -l select=1 -A cin_visual -W group_list=cin_visual -q dcv_visual -- echo
#dcv_visual=qsub -l walltime=00:00:01 -o /dev/null -e /dev/null -l select=1 -q dcv_visual -- echo
course=sbatch --reservation=s_tra_cgpB2018 --partition=gll_usr_gpuprod  --time=00:00:01 --output /dev/null  -A train_cgpB2018  -N 1 -n 1   --wrap "/usr/bin/echo "
course_1gpu=sbatch --reservation=s_tra_cgpB2018 --partition=gll_usr_gpuprod  --time=00:00:01 --output /dev/null  -A train_cgpB2018  -N 1 -n 1   --wrap "/usr/bin/echo "
course_2gpu=sbatch --reservation=s_tra_cgpB2018 --partition=gll_usr_gpuprod  --time=00:00:01 --output /dev/null  -A train_cgpB2018  -N 1 -n 1   --wrap "/usr/bin/echo "
course_4gpu=sbatch --reservation=s_tra_cgpB2018 --partition=gll_usr_gpuprod  --time=00:00:01 --output /dev/null  -A train_cgpB2018  -N 1 -n 1   --wrap "/usr/bin/echo "
1core_2_Gb_3h=sbatch --partition gll_all_serial  --time=00:00:01 --output /dev/null  ${RCM_QSUBPAR_A}  -N 1 -n 1   --wrap "/usr/bin/echo "
4core_24_Gb_12h_gpuprod=sbatch --partition gll_usr_gpuprod  --time=00:00:01 --output /dev/null  ${RCM_QSUBPAR_A}  -N 1 -n 1   --wrap "/usr/bin/echo "
fullnode_3h_gpuprod=sbatch --partition gll_usr_gpuprod  --time=00:00:01 --output /dev/null  ${RCM_QSUBPAR_A}  -N 1 -n 1   --wrap "/usr/bin/echo "
fullnode_3h_gpudev=sbatch --partition gll_usr_gpudev --qos=gll_qos_gpudev --time=00:00:01 --output /dev/null  ${RCM_QSUBPAR_A}  -N 1 -n 1   --wrap "/usr/bin/echo "
fullnode_3h_prod=sbatch --partition gll_usr_prod  --time=00:00:01 --output /dev/null  ${RCM_QSUBPAR_A}  -N 1 -n 1   --wrap "/usr/bin/echo "


[walltimelimit]
minimal_1Gb_1core=12:00:00
light_2Gb_1cor=12:00:00
medium_8Gb_1core=10:00:00
med_16Gb_2core=8:00:00
cpu_8Gb_8core=6:00:00
alarge_32Gb_4core=6:00:00
xtralarge_64Gb_8c=2:00:00
zfull_120Gb_16c=1:00:00
visualrcm=12:00:00
dcv_visual=12:00:00
course=3:00:00
course_1gpu=1:00:00
course_2gpu=1:00:00
course_4gpu=1:00:00
1core_2_Gb_3h=3:00:00
4core_24_Gb_12h=12:00:00
4core_24_Gb_12h_gpuprod=12:00:00
fullnode_3h_gpuprod=3:00:00
fullnode_3h_gpudev=3:00:00
fullnode_3h_prod=3:00:00

[130.186.17]
#login02.galileo.cineca.it=login.galileo.cineca.it
node170.galileo.cineca.it=login06.galileo.cineca.it
node171.galileo.cineca.it=login07.galileo.cineca.it
node172.galileo.cineca.it=login08.galileo.cineca.it
node165.galileo.cineca.it=login01.galileo.cineca.it
#node165.galileo.cineca.it=login.galileo.cineca.it
node166.galileo.cineca.it=login.galileo.cineca.it
#login01.galileo.cineca.it=login.galileo.cineca.it
#login02.galileo.cineca.it=login.galileo.cineca.it



[10.139.7]
node170.galileo.cineca.it=login06-galileo.cineca.pri
node171.galileo.cineca.it=login07-galileo.cineca.pri
node172.galileo.cineca.it=login08-galileo.cineca.pri



